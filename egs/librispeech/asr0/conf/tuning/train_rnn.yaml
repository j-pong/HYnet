# network architecture
# encoder related
etype: bgru     # encoder architecture type
elayers: 5
eunits: 1024
eprojs: 1024
subsample: "1_2_2_1_1" # skip every n frame from input to nth layers

# hybrid CTC/attention
mtlalpha: 0.0

# minibatch related
batch-size: 64
maxlen-in: 800  # if input length  > maxlen_in, batchsize is automatically reduced
maxlen-out: 150 # if output length > maxlen_out, batchsize is automatically reduced

# other config
dropout-rate: 0.3
lnorm: False
bnorm: True

# optimization related
sortagrad: -1 # Feed samples from shortest to longest ; -1: enabled for all epochs, 0: disabled, other: enabled for 'other' epochs
opt: adadelta
epochs: 30
patience: 3

# scheduled sampling option
sampling-probability: 0.0

# transformer specific setting
backend: pytorch
model-module: "moneynet.nets.pytorch_backend.e2e_asr_rnn:E2E"

# initialization method
initializer: lecun
