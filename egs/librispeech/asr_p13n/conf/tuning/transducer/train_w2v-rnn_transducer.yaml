# The conformer transducer training configuration
# WERs for test-clean/test-other are 2.8 and 6.6, respectively.
# Trained with RTX A6000(48GB) x 4 GPUs. It takes about 6 days.
batch_type: numel
batch_bins: 10000000
accum_grad: 4
max_epoch: 25
patience: none
init: none
num_att_plot: 0

val_scheduler_criterion:
    - valid
    - loss
best_model_criterion:
-   - valid
    - loss
    - min
keep_nbest_models: 10

model_conf:
    ctc_weight: 0.3
    report_cer: False
    report_wer: False

encoder: wav2vec2
encoder_conf:
    output_size: 256
    w2v_url: https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_small.pt
    w2v_dir_path: ./w2v_pt_model/
    normalize_before: False
    freeze_finetune_updates: 0

decoder: transducer
decoder_conf:
    rnn_type: lstm
    num_layers: 1
    hidden_size: 512
    dropout: 0.1
    dropout_embed: 0.2

joint_net_conf:
    joint_space_size: 640

optim: adam
optim_conf:
    lr: 0.0015
    weight_decay: 0.000001
scheduler: warmuplr
scheduler_conf:
    warmup_steps: 25000

frontend: s3prl
frontend_conf:
    frontend_conf:
        upstream: wav2vec2
    download_dir: ./models/

preencoder: linear
preencoder_conf:
    input_size: 256
    output_size: 256

# specaug: specaug
# specaug_conf:
#     apply_time_warp: true
#     time_warp_window: 5
#     time_warp_mode: bicubic
#     apply_freq_mask: true
#     freq_mask_width_range:
#     - 0
#     - 30
#     num_freq_mask: 2
#     apply_time_mask: true
#     time_mask_width_range:
#     - 0
#     - 40
#     num_time_mask: 2